{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65324005",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Deep Q-Networks exploration \n",
    "q networks and q learning interchangeable\n",
    "\n",
    "q learning only works for discrete action spaces, continuous action spaces require diff algorithms\n",
    "theory:\n",
    "https://www.youtube.com/watch?v=0bt0SjbS3xc&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=13&ab_channel=deeplizard\n",
    "\n",
    "code:\n",
    "https://www.youtube.com/watch?v=NP8pXZdU-5U&ab_channel=brthor (simpler)\n",
    "https://www.youtube.com/watch?v=wc-FxNENg9U&ab_channel=MachineLearningwithPhil (more advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e73a11",
   "metadata": {
    "hidden": true
   },
   "source": [
    "instead of using value iteration, use deep neural network\n",
    "the network approximates the q function using the bellman eqn\n",
    "\n",
    "n_outputs of DQN = n_actions\n",
    "each node represents the q value for a particular action given the state (which is the input to the DQN) \n",
    "output layer is without activation function so we can see the raw q values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f66cb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "experience replay \n",
    "With deep Q-networks, we often utilize this technique called experience replay during training. With experience replay, we store the agent's experiences at each time step in a data set called the replay memory. We represent the agent's experience at time as.At time, the agent's experience, is defined as this tuple:\n",
    "et=(st,at,rt+1,st+1)\n",
    "\n",
    "\n",
    "A key reason for using replay memory is to break the correlation between consecutive samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d4c35",
   "metadata": {
    "hidden": true
   },
   "source": [
    "DQN Pseudocode without Target Network\n",
    "1. Initialize replay memory capacity.\n",
    "2. Initialize the network with random weights.\n",
    "3. For each episode:\n",
    "- Initialize the starting state.\n",
    "- For each time step:\n",
    "    -Select an action.\n",
    "        -Via exploration or exploitation\n",
    "    -Execute selected action in an emulator.\n",
    "    -Observe reward and next state.\n",
    "    -Store experience in replay memory.\n",
    "    -Sample random batch from replay memory.\n",
    "    -Preprocess states from batch.\n",
    "    -Pass batch of preprocessed states to policy network.\n",
    "    -Calculate loss between output Q-values and target Q-values.\n",
    "        -Requires a second pass to the network for the next state to get the max q value for the next state to get target Q value [inefficient so we introduce a Target Network - freezes the policy networks weights and update every few steps only ]\n",
    "    -Gradient descent updates weights in the policy network to minimize loss.\n",
    "    \n",
    "    \n",
    "DQN Pseudocode without Target Network\n",
    "1. Initialize replay memory capacity.\n",
    "2. Initialize the network with random weights.\n",
    "3. Clone the policy network, and call it the target network.\n",
    "4. For each episode:\n",
    "- Initialize the starting state.\n",
    "- For each time step:\n",
    "    -Select an action.\n",
    "        -Via exploration or exploitation\n",
    "    -Execute selected action in an emulator.\n",
    "    -Observe reward and next state.\n",
    "    -Store experience in replay memory.\n",
    "    -Sample random batch from replay memory.\n",
    "    -Preprocess states from batch.\n",
    "    -Pass batch of preprocessed states to policy network.\n",
    "    -Calculate loss between output Q-values and target Q-values.\n",
    "        -Requires a pass to the target network for the next state\n",
    "    -Gradient descent updates weights in the policy network to minimize loss.\n",
    "        -After time steps, weights in the target network are updated to the weights in the policy network.    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83507a7d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# algo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d596e76",
   "metadata": {
    "hidden": true
   },
   "source": [
    "implementation of the DeepMind paper\n",
    "https://training.incf.org/sites/default/files/2023-05/Human-level%20control%20through%20deep%20reinforcement%20learning.pdf\n",
    "    \n",
    "code from\n",
    "https://www.youtube.com/watch?v=NP8pXZdU-5U&ab_channel=brthor (simpler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1b4c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T13:28:00.541909Z",
     "start_time": "2023-07-06T13:27:57.898283Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c604b8ef",
   "metadata": {
    "hidden": true
   },
   "source": [
    "from collections import deque\n",
    "A deque is a data structure that allows insertion and removal of elements from both ends. This is different from a queue, which only allows insertion at one end and removal from the other end, following a first-in, first-out (FIFO) order.\n",
    "This is a linked list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c73a852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T13:28:00.557954Z",
     "start_time": "2023-07-06T13:28:00.545431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GAMMA=0.99\n",
    "BATCH_SIZE=32 #num transitions to sample from replay buffer\n",
    "BUFFER_SIZE=50000 #max num of transitions to store before overwriting all transitions\n",
    "MIN_REPLAY_SIZE=1000 #min num of transitions to store before computing gradients\n",
    "EPSILON_START=1.0\n",
    "EPSLION_END=0.02\n",
    "EPSILON_DECAY=10000 #num of steps to decay epsilon from start to end, this is NOT the decay value itself but num of steps\n",
    "TARGET_UPDATE_FREQ=1000 #num steps to set target params (target network) to online params (main network)\n",
    "LR=5e-4\n",
    "MAX_STEPS=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39170762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T13:28:00.572956Z",
     "start_time": "2023-07-06T13:28:00.560957Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        in_features=int(np.prod(env.observation_space.shape))\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(in_features,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,env.action_space.n)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def act(self,obs):\n",
    "        state_t=torch.as_tensor(state,dtype=torch.float32) #_t indicates that its tensor, smart easy trick for debugging\n",
    "        q_values=self(state_t.unsqueeze(0)) #add dim to beginning of shape, every operation in pytorch requires batch dim, so unsqueeze(0) adds dim 1 since this state_t only has 1 dim\n",
    "        \n",
    "        max_q_index=torch.argmax(q_values,dim=1)[0]\n",
    "        action=max_q_index.detach().item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26944415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T13:33:09.061309Z",
     "start_time": "2023-07-06T13:29:45.202256Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################# START TRAINING #################\n",
      "\n",
      "Step 0, Avg Rew : 0.0, Time elapsed: 0.02s\n",
      "\n",
      "Step 5000, Avg Rew : 32.47, Time elapsed: 9.02s\n",
      "\n",
      "Step 10000, Avg Rew : 68.38, Time elapsed: 19.27s\n",
      "\n",
      "Step 15000, Avg Rew : 109.28, Time elapsed: 28.91s\n",
      "\n",
      "Step 20000, Avg Rew : 150.3, Time elapsed: 38.81s\n",
      "\n",
      "Step 25000, Avg Rew : 184.7, Time elapsed: 48.47s\n",
      "\n",
      "Step 30000, Avg Rew : 198.21, Time elapsed: 58.12s\n",
      "\n",
      "Step 35000, Avg Rew : 198.65, Time elapsed: 68.43s\n",
      "\n",
      "Step 40000, Avg Rew : 194.37, Time elapsed: 79.20s\n",
      "\n",
      "Step 45000, Avg Rew : 181.65, Time elapsed: 90.12s\n",
      "\n",
      "Step 50000, Avg Rew : 170.05, Time elapsed: 100.35s\n",
      "\n",
      "Step 55000, Avg Rew : 162.28, Time elapsed: 111.53s\n",
      "\n",
      "Step 60000, Avg Rew : 166.59, Time elapsed: 122.31s\n",
      "\n",
      "Step 65000, Avg Rew : 174.38, Time elapsed: 133.05s\n",
      "\n",
      "Step 70000, Avg Rew : 182.3, Time elapsed: 143.36s\n",
      "\n",
      "Step 75000, Avg Rew : 188.02, Time elapsed: 153.18s\n",
      "\n",
      "Step 80000, Avg Rew : 190.01, Time elapsed: 162.95s\n",
      "\n",
      "Step 85000, Avg Rew : 192.81, Time elapsed: 173.07s\n",
      "\n",
      "Step 90000, Avg Rew : 194.57, Time elapsed: 182.97s\n",
      "\n",
      "Step 95000, Avg Rew : 195.34, Time elapsed: 193.41s\n",
      "################# END TRAINING #################\n",
      "Total time elapsed: 203.83s\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('CartPole-v0')\n",
    "\n",
    "replay_buffer=deque(maxlen=BUFFER_SIZE)\n",
    "rew_buffer=deque([0.0],maxlen=100)\n",
    "\n",
    "episode_reward=0.0\n",
    "\n",
    "online_network=Network(env)\n",
    "target_network=Network(env)\n",
    "#need to set the target network params to online network params because they have been defined differently\n",
    "target_network.load_state_dict(online_network.state_dict()) \n",
    "\n",
    "optimizer=torch.optim.Adam(online_network.parameters(),lr=LR)\n",
    "\n",
    "#Initialise replay buffer - only run once at the start of algo for initialisation purpose\n",
    "state=env.reset()\n",
    "start_time=time.time()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action=env.action_space.sample()\n",
    "    new_state,reward,done,_=env.step(action)\n",
    "    transition=(state,action,reward,done,new_state)\n",
    "    replay_buffer.append(transition)\n",
    "    state=new_state\n",
    "    \n",
    "    if done:\n",
    "        state=env.reset()\n",
    "        \n",
    "#main training loop\n",
    "state=env.reset()\n",
    "print('################# START TRAINING #################')\n",
    "for step in range(MAX_STEPS): #epsilon greedy method - need to calc epsilon value\n",
    "    epsilon=np.interp(step,[0,EPSILON_DECAY],[EPSILON_START,EPSLION_END])\n",
    "    \n",
    "    random_sample=random.random()\n",
    "    \n",
    "    if random_sample<=epsilon:\n",
    "        action=env.action_space.sample()\n",
    "    else:\n",
    "        action=online_network.act(state)\n",
    "        \n",
    "    new_state,reward,done,_=env.step(action)\n",
    "    transition=(state,action,reward,done,new_state)\n",
    "    replay_buffer.append(transition)\n",
    "    state=new_state\n",
    "    episode_reward+=reward\n",
    "    \n",
    "    if done:\n",
    "        state=env.reset()\n",
    "        rew_buffer.append(episode_reward)\n",
    "        episode_reward=0.0\n",
    "        \n",
    "#     #After task is solved, test algo on env\n",
    "#     if len(rew_buffer)>=100:\n",
    "#         if np.mean(rew_buffer)>=195:\n",
    "#             while True: #infinite loop\n",
    "#                 action=online_network.act(state)\n",
    "#                 state,_,done,_=env.step(action)\n",
    "#                 env.render()\n",
    "#                 if done:\n",
    "#                     env.reset()\n",
    "        \n",
    "    #start gradient step \n",
    "    transitions=random.sample(replay_buffer,BATCH_SIZE)\n",
    "    \n",
    "    states=np.asarray([t[0] for t in transitions])\n",
    "    actions=np.asarray([t[1] for t in transitions])\n",
    "    rewards=np.asarray([t[2] for t in transitions])    \n",
    "    dones=np.asarray([t[3] for t in transitions])    \n",
    "    new_states=np.asarray([t[4] for t in transitions])    \n",
    "    \n",
    "    states_t=torch.tensor(states,dtype=torch.float32)\n",
    "    actions_t=torch.tensor(actions,dtype=torch.int64).unsqueeze(-1) #unsqueeze(-1) since the var here alr in batches so -1 addes dim to end rather than beginning\n",
    "    rewards_t=torch.tensor(rewards,dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t=torch.tensor(dones,dtype=torch.float32).unsqueeze(-1)\n",
    "    new_states_t=torch.tensor(new_states,dtype=torch.float32)   \n",
    "    \n",
    "    #compute targets\n",
    "    target_q_values=target_network(new_states_t) #get the q values for each state in new_states in the target network\n",
    "    max_target_q_values=target_q_values.max(dim=1,keepdim=True)[0] #get only the highest q values for each state in new state\n",
    "    targets=rewards_t+GAMMA*(1-dones_t)*max_target_q_values #formula from paper\n",
    "    \n",
    "    #compute loss\n",
    "    q_values=online_network(states_t) #get the q values for each state in states in online network\n",
    "    action_q_values=torch.gather(input=q_values,dim=1,index=actions_t) #get q values for specific actions\n",
    "    loss=nn.functional.smooth_l1_loss(action_q_values,targets) #Huber loss\n",
    "    \n",
    "    #gradient descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #update target network\n",
    "    if step%TARGET_UPDATE_FREQ==0:\n",
    "        target_network.load_state_dict(online_network.state_dict())\n",
    "        \n",
    "    #logging\n",
    "    if step%5000==0:\n",
    "        print()\n",
    "        print(f'Step {step}, Avg Rew : {np.mean(rew_buffer)}, Time elapsed: {(time.time()-start_time):.2f}s')\n",
    "        \n",
    "print('################# END TRAINING #################')\n",
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26f471b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T14:24:44.240270Z",
     "start_time": "2023-07-06T14:24:27.331200Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m         state,_,done,_\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m----> 9\u001b[0m             \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\core.py:240\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:213\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcarttrans\u001b[38;5;241m.\u001b[39mset_translation(cartx, carty)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoletrans\u001b[38;5;241m.\u001b[39mset_rotation(\u001b[38;5;241m-\u001b[39mx[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_rgb_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:110\u001b[0m, in \u001b[0;36mViewer.render\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m geom \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeoms:\n\u001b[1;32m--> 110\u001b[0m     \u001b[43mgeom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m geom \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monetime_geoms:\n\u001b[0;32m    112\u001b[0m     geom\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:180\u001b[0m, in \u001b[0;36mGeom.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs):\n\u001b[0;32m    179\u001b[0m     attr\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[0;32m    182\u001b[0m     attr\u001b[38;5;241m.\u001b[39mdisable()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:249\u001b[0m, in \u001b[0;36mFilledPolygon.render1\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender1\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m   \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m : \u001b[43mglBegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGL_QUADS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv)  \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m : glBegin(GL_POLYGON)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: glBegin(GL_TRIANGLES)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pyglet\\gl\\lib.py:111\u001b[0m, in \u001b[0;36merrcheck_glbegin\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m GLException(msg)\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrcheck_glbegin\u001b[39m(result, func, arguments):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gl\n\u001b[0;32m    113\u001b[0m     context \u001b[38;5;241m=\u001b[39m gl\u001b[38;5;241m.\u001b[39mcurrent_context\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_steps=10090\n",
    "n_eps=5\n",
    "for _ in range(n_eps):\n",
    "    state=env.reset()\n",
    "    for _ in range(max_steps): #test 5 episodes\n",
    "        action=online_network.act(state)\n",
    "        state,_,done,_=env.step(action)\n",
    "        if done:\n",
    "            env.render()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bd58890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-06T14:39:02.092062Z",
     "start_time": "2023-07-06T14:39:02.058538Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fe228",
   "metadata": {},
   "source": [
    "# algo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b8f94",
   "metadata": {},
   "source": [
    "code from\n",
    "https://www.youtube.com/watch?v=wc-FxNENg9U&ab_channel=MachineLearningwithPhil \n",
    "\n",
    "only Online Network (DQN), doesnt implement Target Network but the model can still work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b201fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T03:21:52.814930Z",
     "start_time": "2023-07-07T03:21:52.799840Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a0e67b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T03:54:03.665822Z",
     "start_time": "2023-07-07T03:54:03.623825Z"
    }
   },
   "outputs": [],
   "source": [
    "#put this in a python file .py\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self,lr,input_dims,fc1_dims,fc2_dims,n_actions):\n",
    "        super(DeepQNetwork,self).__init__()\n",
    "        self.input_dims=input_dims\n",
    "        self.fc1_dims=fc1_dims\n",
    "        self.fc2_dims=fc2_dims\n",
    "        self.n_actions=n_actions\n",
    "        self.fc1=nn.Linear(*self.input_dims,self.fc1_dims)\n",
    "        self.fc2=nn.Linear(self.fc1_dims,self.fc2_dims)\n",
    "        self.fc3=nn.Linear(self.fc2_dims,self.n_actions)\n",
    "        self.optimizer=optim.Adam(self.parameters(),lr=lr)\n",
    "        self.loss=nn.MSELoss()\n",
    "        self.device=device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x=F.relu(self.fc1(state))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        actions=self.fc3(x)\n",
    "        return actions\n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self,gamma,epsilon,lr,input_dims,batch_size,n_actions,max_mem_size=100000,eps_min=0.01,eps_decay=5e-4):\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.lr=lr\n",
    "        self.input_dims=input_dims\n",
    "        self.batch_size=batch_size\n",
    "        self.n_actions=n_actions\n",
    "        self.mem_size=max_mem_size\n",
    "        self.eps_min=eps_min\n",
    "        self.eps_decay=eps_decay\n",
    "        self.action_space=[i for i in range(n_actions)]\n",
    "        self.mem_counter=0 #keep track of first avail memory for Agent\n",
    "        \n",
    "        self.Q_eval=DeepQNetwork(lr=self.lr,n_actions=self.n_actions,input_dims=self.input_dims,fc1_dims=256,fc2_dims=256)\n",
    "        self.state_memory=np.zeros((self.mem_size,*input_dims),dtype=np.float32)\n",
    "        self.new_state_memory=np.zeros((self.mem_size,*input_dims),dtype=np.float32)\n",
    "        self.action_memory=np.zeros(self.mem_size,dtype=np.int32)\n",
    "        self.reward_memory=np.zeros(self.mem_size,dtype=np.float32)        \n",
    "        self.terminal_memory=np.zeros(self.mem_size,dtype=np.bool) \n",
    "        \n",
    "    def store_transition(self,state,action,reward,new_state,done):\n",
    "        index=self.mem_counter%self.mem_size\n",
    "        self.state_memory[index]=state\n",
    "        self.new_state_memory[index]=new_state        \n",
    "        self.action_memory[index]=action \n",
    "        self.reward_memory[index]=reward       \n",
    "        self.terminal_memory[index]=done\n",
    "        \n",
    "        self.mem_counter+=1\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        if np.random.random()>self.epsilon:\n",
    "            state=torch.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions=self.Q_eval.forward(state)\n",
    "            action=T.argmax(actions).item()\n",
    "        else:\n",
    "            action=np.random.choice(self.action_space)\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.mem_counter<self.batch_size:\n",
    "            return \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        max_mem=min(self.mem_counter,self.mem_size)\n",
    "        batch=np.random.choice(max_mem,self.batch_size,replace=False)\n",
    "        batch_index=np.arange(self.batch_size,dtype=np.int32)\n",
    "        \n",
    "        state_batch=torch.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch=torch.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)        \n",
    "        reward_batch=torch.tensor(self.reward_memory[batch]).to(self.Q_eval.device)        \n",
    "        terminal_batch=torch.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)       \n",
    "        action_batch=self.action_memory[batch]\n",
    "        \n",
    "        q_eval=self.Q_eval.forward(state_batch)[batch_index,action_batch]\n",
    "        q_next=self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch]=0.0\n",
    "        q_target=reward_batch+self.gamma+torch.max(q_next,dims=1)[0]\n",
    "        \n",
    "        loss=self.Q_eval.loss(q_target,q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon=self.epsilon-self.eps_decay if self.epsilon>self.eps_min else self.eps_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then run this code\n",
    "\n",
    "import gym\n",
    "from simple_dqn_torch_2020 import Agent\n",
    "from utils import plotLearning\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4, eps_end=0.01,\n",
    "                  input_dims=[8], lr=0.001)\n",
    "    scores, eps_history = [], []\n",
    "    n_games = 500\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, \n",
    "                                    observation_, done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "\n",
    "        print('episode ', i, 'score %.2f' % score,\n",
    "                'average score %.2f' % avg_score,\n",
    "                'epsilon %.2f' % agent.epsilon)\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    filename = 'lunar_lander.png'\n",
    "    plotLearning(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "    ax2.scatter(x, running_avg, color=\"C1\")\n",
    "    #ax2.xaxis.tick_top()\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    #ax2.set_xlabel('x label 2', color=\"C1\")\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    #ax2.xaxis.set_label_position('top')\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    #ax2.tick_params(axis='x', colors=\"C1\")\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "class SkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(SkipEnv, self).__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        t_reward = 0.0\n",
    "        done = False\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            t_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, t_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer = []\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "class PreProcessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(PreProcessFrame, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "                                                shape=(80,80,1), dtype=np.uint8)\n",
    "    def observation(self, obs):\n",
    "        return PreProcessFrame.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "\n",
    "        new_frame = np.reshape(frame, frame.shape).astype(np.float32)\n",
    "\n",
    "        new_frame = 0.299*new_frame[:,:,0] + 0.587*new_frame[:,:,1] + \\\n",
    "                    0.114*new_frame[:,:,2]\n",
    "\n",
    "        new_frame = new_frame[35:195:2, ::2].reshape(80,80,1)\n",
    "\n",
    "        return new_frame.astype(np.uint8)\n",
    "\n",
    "class MoveImgChannel(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(MoveImgChannel, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
    "                            shape=(self.observation_space.shape[-1],\n",
    "                                   self.observation_space.shape[0],\n",
    "                                   self.observation_space.shape[1]),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "class ScaleFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                             env.observation_space.low.repeat(n_steps, axis=0),\n",
    "                             env.observation_space.high.repeat(n_steps, axis=0),\n",
    "                             dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=np.float32)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = SkipEnv(env)\n",
    "    env = PreProcessFrame(env)\n",
    "    env = MoveImgChannel(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaleFrame(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e51e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c2ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e692018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a363c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003f122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
