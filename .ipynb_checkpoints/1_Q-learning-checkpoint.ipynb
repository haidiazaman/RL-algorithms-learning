{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a44985",
   "metadata": {},
   "source": [
    "# OpenAI cartpole basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdc521",
   "metadata": {},
   "source": [
    "https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
    "\n",
    "https://www.gymlibrary.dev/environments/classic_control/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0e52bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T13:42:00.392944Z",
     "start_time": "2023-07-04T13:41:38.274030Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym==0.17.3\n",
      "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ----------- ---------------------------- 0.5/1.6 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.2/1.6 MB 15.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 14.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy in c:\\users\\palaash.hpz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from gym==0.17.3) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\palaash.hpz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from gym==0.17.3) (1.24.1)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "     ---------------------- ----------------- 0.6/1.0 MB 12.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.0/1.0 MB 13.0 MB/s eta 0:00:00\n",
      "Collecting cloudpickle<1.7.0,>=1.2.0\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting future\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "     ---------------------------------------- 0.0/840.9 kB ? eta -:--:--\n",
      "     ------------------------------------  839.7/840.9 kB 26.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- 840.9/840.9 kB 17.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654631 sha256=f1dd70a72e08688b1d819504596b15777f2be5740bb08d0dccb746175f2ff765\n",
      "  Stored in directory: c:\\users\\palaash.hpz\\appdata\\local\\pip\\cache\\wheels\\af\\4b\\74\\fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492055 sha256=7db5ddbf0258b336f6a1e6496b546bc07b13634b21f1037dc8297fc30b2241d1\n",
      "  Stored in directory: c:\\users\\palaash.hpz\\appdata\\local\\pip\\cache\\wheels\\5e\\a9\\47\\f118e66afd12240e4662752cc22cefae5d97275623aa8ef57d\n",
      "Successfully built gym future\n",
      "Installing collected packages: future, cloudpickle, pyglet, gym\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.1\n",
      "    Uninstalling cloudpickle-2.2.1:\n",
      "      Successfully uninstalled cloudpickle-2.2.1\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.26.2\n",
      "    Uninstalling gym-0.26.2:\n",
      "      Successfully uninstalled gym-0.26.2\n",
      "Successfully installed cloudpickle-1.6.0 future-0.18.3 gym-0.17.3 pyglet-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.17.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424424c",
   "metadata": {},
   "source": [
    "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "Num Action (apply a force)\n",
    "\n",
    "0  Push cart to the left\n",
    "\n",
    "1  Push cart to the right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d43cf",
   "metadata": {},
   "source": [
    "rewards\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 475 for v1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501e0ec",
   "metadata": {},
   "source": [
    "episode is max of 10s, if can remain upright thru the entire time then episode done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5eebfe",
   "metadata": {},
   "source": [
    "Episode End\n",
    "The episode ends if any one of the following occurs:\n",
    "\n",
    "Termination: Pole Angle is greater than ±12°\n",
    "\n",
    "Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "\n",
    "Truncation: Episode length is greater than 500 (200 for v0)\n",
    "    \n",
    "Starting State\n",
    "All observations are assigned a uniformly random value in (-0.05, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e787eb",
   "metadata": {},
   "source": [
    "Arguments\n",
    "gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b0e1bcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T13:49:46.449121Z",
     "start_time": "2023-07-04T13:49:32.132541Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Palaash.HPZ\\AppData\\Local\\Temp\\ipykernel_6028\\3641482266.py\", line 9, in <module>\n",
      "    env.step(action)\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\wrappers\\time_limit.py\", line 50, in step\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py\", line 37, in step\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\wrappers\\env_checker.py\", line 39, in step\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\", line 187, in step\n",
      "    pole.add_attr(self.carttrans)\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\", line 298, in render\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1',render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "# Display the environment - will pop up in a window in a separate env\n",
    "while True:\n",
    "    action=env.action_space.sample()\n",
    "    env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee2e33e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-04T13:49:49.239010Z",
     "start_time": "2023-07-04T13:49:49.186863Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a2267",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q-learning exploration\n",
    "on OpenAI Frozen-Lake\n",
    "\n",
    "DeepLizard q learning algo tutorial\n",
    "https://www.youtube.com/watch?v=HGeI30uATws&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=9&ab_channel=deeplizard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536fd304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T04:34:19.786715Z",
     "start_time": "2023-07-05T04:34:14.601531Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\palaash.hpz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\palaash.hpz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\palaash.hpz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from gym) (1.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\palaash.hpz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from gym) (1.10.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\palaash.hpz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in c:\\users\\palaash.hpz\\anaconda3\\envs\\pytorch_env\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac859270",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Frozen Lake\n",
    "I've grabbed the description of the game directly from Gym's website. Let's read through it together.\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following:\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "This grid is our environment where S is the agent's starting point, and it's safe. F represents the frozen surface and is also safe. H represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that's not good. Finally, G represents the goal, which is the space on the grid where the prized frisbee is located.\n",
    "\n",
    "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise.\n",
    "\n",
    "State\tDescription\tReward\n",
    "S\tAgent's starting point - safe\t0\n",
    "F\tFrozen surface - safe\t0\n",
    "H\tHole - game over\t0\n",
    "G\tGoal - game over\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a62c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T04:37:21.856367Z",
     "start_time": "2023-07-05T04:37:21.092761Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4ed24f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T04:37:49.200304Z",
     "start_time": "2023-07-05T04:37:46.672578Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "801fb6fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T05:04:57.735387Z",
     "start_time": "2023-07-05T05:04:57.711690Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space_size=env.action_space.n\n",
    "state_space_size=env.observation_space.n\n",
    "action_space_size,state_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b512b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T07:21:26.511403Z",
     "start_time": "2023-07-05T07:21:26.498252Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table=np.zeros((state_space_size,action_space_size))\n",
    "q_table #cols are actions, rows are state spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb5738b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T06:42:51.690309Z",
     "start_time": "2023-07-05T06:42:51.678203Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_episodes=10000\n",
    "max_steps=100 #per episode\n",
    "\n",
    "lr=0.1\n",
    "gamma=0.99 #discount rate\n",
    "\n",
    "epsilon=1 #exploration rate initialised to 1\n",
    "max_epsilon=1\n",
    "min_epsilon=0.01\n",
    "epsilon_decay_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7692e794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T06:47:41.815039Z",
     "start_time": "2023-07-05T06:47:27.800627Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.1300000000000001\n",
      "2000 :  0.4110000000000003\n",
      "3000 :  0.4250000000000003\n",
      "4000 :  0.5830000000000004\n",
      "5000 :  0.6960000000000005\n",
      "6000 :  0.6830000000000005\n",
      "7000 :  0.6830000000000005\n",
      "8000 :  0.6500000000000005\n",
      "9000 :  0.6930000000000005\n",
      "10000 :  0.7100000000000005\n",
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.55286845 0.51268178 0.49434934 0.50160874]\n",
      " [0.08360022 0.11746611 0.15124224 0.49224999]\n",
      " [0.39742406 0.19640409 0.17165541 0.20788522]\n",
      " [0.11904098 0.         0.         0.        ]\n",
      " [0.56502277 0.45482487 0.35033856 0.36754067]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34978046 0.12790535 0.16617634 0.09619136]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33754695 0.44879802 0.40354035 0.60304211]\n",
      " [0.45907248 0.67378309 0.54589134 0.43403797]\n",
      " [0.64578107 0.24956514 0.34572227 0.39508101]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40154229 0.56086349 0.80261477 0.45295924]\n",
      " [0.70310792 0.9098597  0.76332636 0.77153007]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes=[]\n",
    "\n",
    "env=gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state=env.reset()\n",
    "    \n",
    "    done=False\n",
    "    rewards_current=0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #exploration vs exploitation trade off\n",
    "        epsilon_random_num=random.uniform(0,1) #generate random number and compare to epsilon to see whether to explore or exploit\n",
    "        if epsilon_random_num>epsilon:\n",
    "            action=np.argmax(q_table[state,:]) #exploit - get the action with the best q value for that given state\n",
    "        else:\n",
    "            action=env.action_space.sample() #explore - get a random action to explore the env\n",
    "        \n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        \n",
    "        #update q values\n",
    "        q_table[state,action]=q_table[state,action] * (1-lr) + lr*(reward+gamma*np.max(q_table[new_state,:])) #follow the formula\n",
    "        \n",
    "        state=new_state\n",
    "        rewards_current+=reward\n",
    "        \n",
    "        if done==True:\n",
    "            break\n",
    "            \n",
    "    #exploration rate decay\n",
    "    epsilon=min_epsilon+(max_epsilon-min_epsilon)*np.exp(-epsilon*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current)\n",
    "    \n",
    "    \n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),n_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70bc28",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ave reward of 0.1300000000000001 means the agent hits the target 13% of the time (13%chance of hitting target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79d26053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T07:19:01.863813Z",
     "start_time": "2023-07-05T07:19:01.856205Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# try put the code into functions\n",
    "\n",
    "def train_one_episode(max_steps,state,epsilon,q_table):\n",
    "    done=False\n",
    "    rewards_current_episode=0\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #exploration vs exploitation trade off\n",
    "        epsilon_random_num=random.uniform(0,1) #generate random number and compare to epsilon to see whether to explore or exploit\n",
    "        if epsilon_random_num>epsilon:\n",
    "            action=np.argmax(q_table[state,:]) #exploit - get the action with the best q value for that given state\n",
    "        else:\n",
    "            action=env.action_space.sample() #explore - get a random action to explore the env\n",
    "        \n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        \n",
    "        #update q values - follow the formula\n",
    "        q_table[state,action]=q_table[state,action] * (1-lr) + lr*(reward+gamma*np.max(q_table[new_state,:])) \n",
    "        \n",
    "        state=new_state\n",
    "        rewards_current_episode+=reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        return state,rewards_current_episode,q_table\n",
    "    \n",
    "# training loop\n",
    "\n",
    "n_episodes=10000\n",
    "max_steps=100 #per episode\n",
    "\n",
    "lr=0.1\n",
    "gamma=0.99 #discount rate\n",
    "\n",
    "epsilon=1 #exploration rate initialised to 1\n",
    "max_epsilon=1\n",
    "min_epsilon=0.01\n",
    "epsilon_decay_rate=0.001\n",
    "\n",
    "env=gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "rewards_all_episodes=[]\n",
    "\n",
    "print_count=1000\n",
    "for episode in range(n_episodes):\n",
    "    state=env.reset()\n",
    "    #train 1 episode - update current state, entire current episode rewards, q_table\n",
    "    state,rewards_current_episode,q_table=train_one_episode(max_steps,state,epsilon,q_table) \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "    #exploration rate decay\n",
    "    epsilon=min_epsilon+(max_epsilon-min_epsilon)*np.exp(-epsilon*episode)\n",
    "    \n",
    "    #print results\n",
    "    if (episode+1)%print_count==0:\n",
    "        batch_rewards=rewards_all_episodes[print_count-1000:print_count]\n",
    "        ave_rewards_last_batch=sum(batch_rewards)/1000\n",
    "        print(f'Average rewards for Episode {print_count-1000}-{print_count}: {ave_rewards_last_batch}')\n",
    "        print_count+=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adbed748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T07:32:00.998858Z",
     "start_time": "2023-07-05T07:31:34.687870Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for Episode 0-1000: 0.086, Time elapsed: 0.26s\n",
      "Average reward for Episode 1000-2000: 0.178, Time elapsed: 0.66s\n",
      "Average reward for Episode 2000-3000: 0.399, Time elapsed: 1.52s\n",
      "Average reward for Episode 3000-4000: 0.43, Time elapsed: 2.22s\n",
      "Average reward for Episode 4000-5000: 0.52, Time elapsed: 3.51s\n",
      "Average reward for Episode 5000-6000: 0.662, Time elapsed: 4.40s\n",
      "Average reward for Episode 6000-7000: 0.633, Time elapsed: 5.71s\n",
      "Average reward for Episode 7000-8000: 0.681, Time elapsed: 7.22s\n",
      "Average reward for Episode 8000-9000: 0.667, Time elapsed: 8.65s\n",
      "Average reward for Episode 9000-10000: 0.693, Time elapsed: 10.13s\n",
      "Average reward for Episode 10000-11000: 0.661, Time elapsed: 11.61s\n",
      "Average reward for Episode 11000-12000: 0.681, Time elapsed: 13.18s\n",
      "Average reward for Episode 12000-13000: 0.637, Time elapsed: 14.64s\n",
      "Average reward for Episode 13000-14000: 0.665, Time elapsed: 16.21s\n",
      "Average reward for Episode 14000-15000: 0.668, Time elapsed: 17.64s\n",
      "Average reward for Episode 15000-16000: 0.666, Time elapsed: 19.08s\n",
      "Average reward for Episode 16000-17000: 0.686, Time elapsed: 20.68s\n",
      "Average reward for Episode 17000-18000: 0.714, Time elapsed: 22.19s\n",
      "Average reward for Episode 18000-19000: 0.685, Time elapsed: 23.81s\n",
      "Average reward for Episode 19000-20000: 0.683, Time elapsed: 25.38s\n",
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.52573774 0.51215524 0.51440481 0.51389898]\n",
      " [0.27896313 0.20038318 0.20394993 0.50505023]\n",
      " [0.35925    0.42191577 0.38166924 0.4745343 ]\n",
      " [0.28908989 0.21947773 0.21105359 0.46071346]\n",
      " [0.53999902 0.36871661 0.24077702 0.28939498]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.15529843 0.12559039 0.36876444 0.22644126]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41515243 0.44452895 0.42835219 0.56374465]\n",
      " [0.37936782 0.62561246 0.40070984 0.49336653]\n",
      " [0.66514842 0.43570043 0.40251987 0.19925856]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.42783442 0.58033231 0.7136158  0.47048163]\n",
      " [0.75702173 0.87998551 0.78186584 0.78491144]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env=gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "action_space_size=env.action_space.n\n",
    "state_space_size=env.observation_space.n\n",
    "\n",
    "q_table=np.zeros((state_space_size,action_space_size)) #cols are actions, rows are state spaces\n",
    "\n",
    "\n",
    "n_episodes=20000\n",
    "max_steps=100 #per episode\n",
    "\n",
    "lr=0.1\n",
    "gamma=0.99 #discount rate\n",
    "\n",
    "epsilon=1 #exploration rate initialised to 1\n",
    "max_epsilon=1\n",
    "min_epsilon=0.01\n",
    "epsilon_decay_rate=0.001\n",
    "\n",
    "rewards_all_episodes=[]\n",
    "start_time=time.time()\n",
    "for episode in range(n_episodes):\n",
    "    state=env.reset()\n",
    "    \n",
    "    done=False\n",
    "    rewards_current=0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #exploration vs exploitation trade off\n",
    "        epsilon_random_num=random.uniform(0,1) #generate random number and compare to epsilon to see whether to explore or exploit\n",
    "        if epsilon_random_num>epsilon:\n",
    "            action=np.argmax(q_table[state,:]) #exploit - get the action with the best q value for that given state\n",
    "        else:\n",
    "            action=env.action_space.sample() #explore - get a random action to explore the env\n",
    "        \n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        \n",
    "        #update q values\n",
    "        q_table[state,action]=q_table[state,action] * (1-lr) + lr*(reward+gamma*np.max(q_table[new_state,:])) #follow the formula\n",
    "        \n",
    "        state=new_state\n",
    "        rewards_current+=reward\n",
    "        \n",
    "        if done==True:\n",
    "            break\n",
    "            \n",
    "    #exploration rate decay\n",
    "    epsilon=min_epsilon+(max_epsilon-min_epsilon)*np.exp(-epsilon*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current)\n",
    "    \n",
    "    if (episode+1)%1000==0:\n",
    "        ave_reward_last_thousand_ep=sum(rewards_all_episodes[-1000:])/1000\n",
    "        print(f'Average reward for Episode {episode+1-1000}-{episode+1}: {ave_reward_last_thousand_ep}, Time elapsed: {(time.time()-start_time):.2f}s')\n",
    "    \n",
    "    \n",
    "# # Calculate and print the average reward per thousand episodes\n",
    "# rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),n_episodes/1000)\n",
    "# count = 1000\n",
    "\n",
    "# print(\"********Average reward per thousand episodes********\\n\")\n",
    "# for r in rewards_per_thousand_episodes:\n",
    "#     print(count, \": \", str(sum(r/1000)))\n",
    "#     count += 1000\n",
    "    \n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2b69c",
   "metadata": {},
   "source": [
    "# Q-learning - full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6fd122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T07:33:44.415024Z",
     "start_time": "2023-07-05T07:32:35.532575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for Episode 0-5000: 0.3964, Time elapsed: 5.67s\n",
      "Average reward for Episode 5000-10000: 0.6624, Time elapsed: 12.40s\n",
      "Average reward for Episode 10000-15000: 0.6504, Time elapsed: 19.32s\n",
      "Average reward for Episode 15000-20000: 0.6736, Time elapsed: 26.45s\n",
      "Average reward for Episode 20000-25000: 0.6722, Time elapsed: 33.68s\n",
      "Average reward for Episode 25000-30000: 0.6828, Time elapsed: 40.55s\n",
      "Average reward for Episode 30000-35000: 0.6646, Time elapsed: 47.56s\n",
      "Average reward for Episode 35000-40000: 0.6796, Time elapsed: 54.72s\n",
      "Average reward for Episode 40000-45000: 0.6744, Time elapsed: 61.73s\n",
      "Average reward for Episode 45000-50000: 0.683, Time elapsed: 68.86s\n",
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.49242512 0.48833038 0.47135847 0.47490835]\n",
      " [0.2654378  0.30636939 0.33686358 0.44115769]\n",
      " [0.35838883 0.2564736  0.24168812 0.22036891]\n",
      " [0.12514671 0.         0.         0.        ]\n",
      " [0.52177627 0.4403537  0.44295825 0.42620948]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.30510837 0.13451666 0.14524162 0.13973352]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3063457  0.30315443 0.39669017 0.5668798 ]\n",
      " [0.49896528 0.63867522 0.49262136 0.35655553]\n",
      " [0.67849993 0.38598394 0.34153399 0.33063491]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.52101151 0.4781763  0.71725977 0.34662895]\n",
      " [0.75543373 0.92255159 0.71994024 0.76306468]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# full code \n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env=gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "action_space_size=env.action_space.n\n",
    "state_space_size=env.observation_space.n\n",
    "\n",
    "q_table=np.zeros((state_space_size,action_space_size)) #cols are actions, rows are state spaces\n",
    "\n",
    "\n",
    "n_episodes=50000\n",
    "max_steps=100 #per episode\n",
    "\n",
    "lr=0.1\n",
    "gamma=0.99 #discount rate\n",
    "\n",
    "epsilon=1 #exploration rate initialised to 1\n",
    "max_epsilon=1\n",
    "min_epsilon=0.01\n",
    "epsilon_decay_rate=0.001\n",
    "\n",
    "rewards_all_episodes=[]\n",
    "start_time=time.time()\n",
    "for episode in range(n_episodes):\n",
    "    state=env.reset()\n",
    "    \n",
    "    done=False\n",
    "    rewards_current=0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #exploration vs exploitation trade off\n",
    "        epsilon_random_num=random.uniform(0,1) #generate random number and compare to epsilon to see whether to explore or exploit\n",
    "        if epsilon_random_num>epsilon:\n",
    "            action=np.argmax(q_table[state,:]) #exploit - get the action with the best q value for that given state\n",
    "        else:\n",
    "            action=env.action_space.sample() #explore - get a random action to explore the env\n",
    "        \n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        \n",
    "        #update q values\n",
    "        q_table[state,action]=q_table[state,action] * (1-lr) + lr*(reward+gamma*np.max(q_table[new_state,:])) #follow the formula\n",
    "        \n",
    "        state=new_state\n",
    "        rewards_current+=reward\n",
    "        \n",
    "        if done==True:\n",
    "            break\n",
    "            \n",
    "    #exploration rate decay\n",
    "    epsilon=min_epsilon+(max_epsilon-min_epsilon)*np.exp(-epsilon*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current)\n",
    "    \n",
    "    if (episode+1)%5000==0:\n",
    "        ave_reward_last_thousand_ep=sum(rewards_all_episodes[-5000:])/5000\n",
    "        print(f'Average reward for Episode {episode+1-5000}-{episode+1}: {ave_reward_last_thousand_ep}, Time elapsed: {(time.time()-start_time):.2f}s')\n",
    "    \n",
    "    \n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e142ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T07:45:48.970126Z",
     "start_time": "2023-07-05T07:45:18.896387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "None\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "# test the agent with new q table\n",
    "\n",
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps):    \n",
    "        # Show current state of environment on screen\n",
    "        # Choose action with highest Q-value for current state       \n",
    "        # Take new action\n",
    "        clear_output(wait=True)\n",
    "        print(env.render())\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action=np.argmax(q_table[state,:])\n",
    "        new_state,reward,done,info=env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            print(env.render())\n",
    "            if reward == 1: \n",
    "                # Agent reached the goal and won episode\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                # Agent stepped in a hole and lost episode            \n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break       \n",
    "\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d21baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d68473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b1055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0d725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce859a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec154768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63c3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef88151b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d89d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b5f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fe7a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46481c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42853351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
